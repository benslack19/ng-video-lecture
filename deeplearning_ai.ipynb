{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6439201-e52e-4e02-9841-5486fe10ca99",
   "metadata": {},
   "source": [
    "# DeepLearningAI course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5abeca-05bb-459d-8bec-9a0be542aed7",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "- add formatting packages from ds-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769914f3-f0ff-47e9-b3a9-2233ad79d9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d633d218-c626-41da-9f8e-62e3e183c5d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T07:50:47.335382Z",
     "iopub.status.busy": "2025-02-15T07:50:47.334353Z",
     "iopub.status.idle": "2025-02-15T07:50:49.897195Z",
     "shell.execute_reply": "2025-02-15T07:50:49.896683Z",
     "shell.execute_reply.started": "2025-02-15T07:50:47.335312Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # to access softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7406843b-7e21-47f0-902b-d5b20fcb6d48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T06:49:27.106093Z",
     "iopub.status.busy": "2025-02-16T06:49:27.105121Z",
     "iopub.status.idle": "2025-02-16T06:49:27.118881Z",
     "shell.execute_reply": "2025-02-16T06:49:27.118153Z",
     "shell.execute_reply.started": "2025-02-16T06:49:27.106055Z"
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=2, row_dim=0, col_dim=1):\n",
    "        \"\"\"\n",
    "        d_model\n",
    "            Dimension of the model or number of word embedding values per token\n",
    "            Will define size of the weight matrices to create queries, keys, values\n",
    "            (needs to match number of word embedding values for the matrix math)\n",
    "\n",
    "        row_dim\n",
    "            convenience parameter to modify row index of data\n",
    "\n",
    "        col_dim\n",
    "            convenience parameter to modify col index of data\n",
    "\n",
    "        Note: Usually first dimenions is batch size but we won't be using batches of data\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()  # to inherit parent's init method\n",
    "\n",
    "        # Create weight for queries, using nn.Linear will create Weight matrix and\n",
    "        # do math for us\n",
    "        # in_features, out_features: number of rows and columns in weight matrix\n",
    "        # but it can be modified as long as matrix math works out\n",
    "        # bias: no additonal bias terms when False\n",
    "        # will create currently untrained weights\n",
    "        # in notes, weights is with transpose symbol because of how pytorch prints weights\n",
    "        \n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        \n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "\n",
    "    def forward(self, token_encodings, return_scaled_sims=False):\n",
    "        \"\"\"Calculate self-attention values for each token\n",
    "\n",
    "        Attention(Q,K,V) = softmax( QK^T / sqrt(d_k)) * V\n",
    "\n",
    "        \"\"\"\n",
    "        #Q = self.W_q @ token_encodings\n",
    "        # this does matrix multiplication because the\n",
    "        # input argument to nn.Linear is data\n",
    "        q = self.W_q(token_encodings)\n",
    "\n",
    "        k = self.W_k(token_encodings)\n",
    "\n",
    "        v = self.W_v(token_encodings)\n",
    "\n",
    "        # torch.matmul = matrix multiply \n",
    "        # get similarities/dot products\n",
    "        unscaled_sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
    "        scaled_sims = unscaled_sims / torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "        # to help visualize attention pattern\n",
    "        if return_scaled_sims:\n",
    "            return scaled_sims\n",
    "\n",
    "        # calculate attention\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "         \n",
    "        return attention_scores\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f87b8441-d47b-46c2-a76f-9072b3c336fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T06:49:28.718210Z",
     "iopub.status.busy": "2025-02-16T06:49:28.717570Z",
     "iopub.status.idle": "2025-02-16T06:49:28.731508Z",
     "shell.execute_reply": "2025-02-16T06:49:28.730689Z",
     "shell.execute_reply.started": "2025-02-16T06:49:28.718175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "selfAttention = SelfAttention(d_model=2, row_dim=0, col_dim=1)\n",
    "\n",
    "# since the model inherits from nn.Module\n",
    "# it will pass the encodings into the forward matrix\n",
    "# giving us attention scores (self-attention values)\n",
    "selfAttention(encodings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51874f73-abc0-43ba-b4b7-d2de74bf2898",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-16T06:49:37.069632Z",
     "iopub.status.busy": "2025-02-16T06:49:37.068669Z",
     "iopub.status.idle": "2025-02-16T06:49:37.082819Z",
     "shell.execute_reply": "2025-02-16T06:49:37.082098Z",
     "shell.execute_reply.started": "2025-02-16T06:49:37.069580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Similarity Scores:\n",
      "tensor([[-0.0700,  0.0458, -0.4612],\n",
      "        [-0.2844,  0.2883, -2.1230],\n",
      "        [ 0.3424, -0.4725,  2.8610]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "scaled_sims = selfAttention(encodings_matrix, return_scaled_sims=True)\n",
    "print(\"Scaled Similarity Scores:\")\n",
    "print(scaled_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a4689c-501f-4e4c-ba97-9ea7b7ad35dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aaebbe7-e8f6-4cc5-be78-2350fcb9a7c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T08:23:07.358367Z",
     "iopub.status.busy": "2025-02-15T08:23:07.356688Z",
     "iopub.status.idle": "2025-02-15T08:23:07.378319Z",
     "shell.execute_reply": "2025-02-15T08:23:07.377717Z",
     "shell.execute_reply.started": "2025-02-15T08:23:07.358305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5406, -0.1657],\n",
       "        [ 0.5869,  0.6496]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate math was done correctly by printing out the weights in the matrix\n",
    "# that we used to calculate\n",
    "# and verify things manually\n",
    "\n",
    "selfAttention.W_q.weight.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23ecaf71-272d-4b8a-a1ef-a10760436556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T08:23:12.987993Z",
     "iopub.status.busy": "2025-02-15T08:23:12.987207Z",
     "iopub.status.idle": "2025-02-15T08:23:12.997981Z",
     "shell.execute_reply": "2025-02-15T08:23:12.997089Z",
     "shell.execute_reply.started": "2025-02-15T08:23:12.987948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1549, -0.3443],\n",
       "        [ 0.1427,  0.4153]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selfAttention.W_k.weight.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39fd7152-be70-44f6-8078-d460659badf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-15T08:23:15.544197Z",
     "iopub.status.busy": "2025-02-15T08:23:15.543518Z",
     "iopub.status.idle": "2025-02-15T08:23:15.554080Z",
     "shell.execute_reply": "2025-02-15T08:23:15.553326Z",
     "shell.execute_reply.started": "2025-02-15T08:23:15.544155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6233,  0.6146],\n",
       "        [-0.5188,  0.1323]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selfAttention.W_v.weight.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da26cc-4edc-420c-b59a-59c8da33e6ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
