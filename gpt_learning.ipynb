{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df4c5739-4f87-4494-88a8-4e603ec1e31a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T14:35:57.594449Z",
     "iopub.status.busy": "2025-02-11T14:35:57.592425Z",
     "iopub.status.idle": "2025-02-11T14:37:27.888739Z",
     "shell.execute_reply": "2025-02-11T14:37:27.888166Z",
     "shell.execute_reply.started": "2025-02-11T14:35:57.594341Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57fba35e-d730-447d-a7fa-cc6e7522a693",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T14:37:27.890393Z",
     "iopub.status.busy": "2025-02-11T14:37:27.890120Z",
     "iopub.status.idle": "2025-02-11T14:37:27.893676Z",
     "shell.execute_reply": "2025-02-11T14:37:27.893192Z",
     "shell.execute_reply.started": "2025-02-11T14:37:27.890375Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30199d4a-6ee9-48a0-9819-25aed9079be0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T15:18:26.705539Z",
     "iopub.status.busy": "2025-02-11T15:18:26.704313Z",
     "iopub.status.idle": "2025-02-11T15:18:26.713571Z",
     "shell.execute_reply": "2025-02-11T15:18:26.712680Z",
     "shell.execute_reply.started": "2025-02-11T15:18:26.705483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22ef1f30-dad2-43f7-8020-62fb689a40ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T15:18:35.026868Z",
     "iopub.status.busy": "2025-02-11T15:18:35.026181Z",
     "iopub.status.idle": "2025-02-11T15:18:35.032764Z",
     "shell.execute_reply": "2025-02-11T15:18:35.031744Z",
     "shell.execute_reply.started": "2025-02-11T15:18:35.026821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a83e81-4840-42d1-b13c-4df14617e52e",
   "metadata": {},
   "source": [
    "# Encoding of characters\n",
    "\n",
    "This is very simple. But there are other methods like `tiktoken`, `sentencepiece` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ce77e60-5206-4bf5-ad03-04abaa0e5d88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T15:18:39.004447Z",
     "iopub.status.busy": "2025-02-11T15:18:39.003352Z",
     "iopub.status.idle": "2025-02-11T15:18:39.030813Z",
     "shell.execute_reply": "2025-02-11T15:18:39.030256Z",
     "shell.execute_reply.started": "2025-02-11T15:18:39.004346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"vocab size: {vocab_size}\")\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }  # string to integer\n",
    "itos = { i:ch for i,ch in enumerate(chars) }  # integer to string\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "796d7f2c-2b8e-46c4-b6f6-57ec882381b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T15:18:58.552293Z",
     "iopub.status.busy": "2025-02-11T15:18:58.551258Z",
     "iopub.status.idle": "2025-02-11T15:18:58.560049Z",
     "shell.execute_reply": "2025-02-11T15:18:58.558760Z",
     "shell.execute_reply.started": "2025-02-11T15:18:58.552221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 53, 1, 40, 43, 1, 53, 56, 1, 52, 53, 58, 1, 58, 53, 1, 40, 43]\n",
      "to be or not to be\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"to be or not to be\"))\n",
    "print(decode(encode(\"to be or not to be\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fa76945-bffb-4129-b10f-4bd15e7b522a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T15:19:01.833937Z",
     "iopub.status.busy": "2025-02-11T15:19:01.832789Z",
     "iopub.status.idle": "2025-02-11T15:19:01.842967Z",
     "shell.execute_reply": "2025-02-11T15:19:01.842191Z",
     "shell.execute_reply.started": "2025-02-11T15:19:01.833847Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '$': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '3': 9,\n",
       " ':': 10,\n",
       " ';': 11,\n",
       " '?': 12,\n",
       " 'A': 13,\n",
       " 'B': 14,\n",
       " 'C': 15,\n",
       " 'D': 16,\n",
       " 'E': 17,\n",
       " 'F': 18,\n",
       " 'G': 19,\n",
       " 'H': 20,\n",
       " 'I': 21,\n",
       " 'J': 22,\n",
       " 'K': 23,\n",
       " 'L': 24,\n",
       " 'M': 25,\n",
       " 'N': 26,\n",
       " 'O': 27,\n",
       " 'P': 28,\n",
       " 'Q': 29,\n",
       " 'R': 30,\n",
       " 'S': 31,\n",
       " 'T': 32,\n",
       " 'U': 33,\n",
       " 'V': 34,\n",
       " 'W': 35,\n",
       " 'X': 36,\n",
       " 'Y': 37,\n",
       " 'Z': 38,\n",
       " 'a': 39,\n",
       " 'b': 40,\n",
       " 'c': 41,\n",
       " 'd': 42,\n",
       " 'e': 43,\n",
       " 'f': 44,\n",
       " 'g': 45,\n",
       " 'h': 46,\n",
       " 'i': 47,\n",
       " 'j': 48,\n",
       " 'k': 49,\n",
       " 'l': 50,\n",
       " 'm': 51,\n",
       " 'n': 52,\n",
       " 'o': 53,\n",
       " 'p': 54,\n",
       " 'q': 55,\n",
       " 'r': 56,\n",
       " 's': 57,\n",
       " 't': 58,\n",
       " 'u': 59,\n",
       " 'v': 60,\n",
       " 'w': 61,\n",
       " 'x': 62,\n",
       " 'y': 63,\n",
       " 'z': 64}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2af7482a-2f90-458a-ae91-665f094097cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T15:19:11.083802Z",
     "iopub.status.busy": "2025-02-11T15:19:11.082774Z",
     "iopub.status.idle": "2025-02-11T15:19:11.292159Z",
     "shell.execute_reply": "2025-02-11T15:19:11.291686Z",
     "shell.execute_reply.started": "2025-02-11T15:19:11.083727Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode and store it in a torch.tensor object\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # torch.long is desired memory format of Tensor\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b75e9125-7e19-42ae-9df0-fac6a1cc3dae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T15:19:28.815163Z",
     "iopub.status.busy": "2025-02-11T15:19:28.814259Z",
     "iopub.status.idle": "2025-02-11T15:19:28.827452Z",
     "shell.execute_reply": "2025-02-11T15:19:28.826444Z",
     "shell.execute_reply.started": "2025-02-11T15:19:28.815092Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `data` is just numbers representing the lyrics\n",
    "decode(encode(text[0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f57184-3ea6-4fe8-89d3-7a95d7ff375f",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "We don't feed the whole training data into the model. Too computationally expensive. Instead, we feed random chunks of data at a time. The size of these chunks is called  `block_size` in this example but others may call it `context_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "590fa106-8d6e-4975-9a27-bf38df414ef7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T23:10:22.735292Z",
     "iopub.status.busy": "2025-02-11T23:10:22.731964Z",
     "iopub.status.idle": "2025-02-11T23:10:22.815445Z",
     "shell.execute_reply": "2025-02-11T23:10:22.814946Z",
     "shell.execute_reply.started": "2025-02-11T23:10:22.735241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c262bb88-892f-462d-88d6-9afbede2cb35",
   "metadata": {},
   "source": [
    "In this particular example, there's actually a lot of examples packed into it because of the masking.\n",
    "\n",
    "`3`\n",
    "<br>\n",
    "`3,  1`\n",
    "<br>\n",
    "`3,  1, 20`  # in the context of seeing `3, 1` then `20` comes next\n",
    "<br>\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2815a945-872c-4e97-bd43-2f5373deaf4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T23:10:25.109742Z",
     "iopub.status.busy": "2025-02-11T23:10:25.108984Z",
     "iopub.status.idle": "2025-02-11T23:10:25.127068Z",
     "shell.execute_reply": "2025-02-11T23:10:25.126113Z",
     "shell.execute_reply.started": "2025-02-11T23:10:25.109688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is: tensor([18]) the target is: 47\n",
      "when input is: tensor([18, 47]) the target is: 56\n",
      "when input is: tensor([18, 47, 56]) the target is: 57\n",
      "when input is: tensor([18, 47, 56, 57]) the target is: 58\n",
      "when input is: tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "when input is: tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "when input is: tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "when input is: tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is: {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890a5435-7b04-4924-9eee-2cd98dead641",
   "metadata": {},
   "source": [
    "This also helps the transformer get used to seeing contexts at these size ranges (from 1 to `block_size`). This is also helpful during inference so the model knows what sizes it can predict. Keep in mind that this also limits what the model can predict. After `block_size` the model has to truncate. The above is implicitly the time dimension, since a sequence where order matters is generated.\n",
    "\n",
    "Now consider the **batch** dimension. It's taking multiple random chunks of text and grouping them together to keep things efficient and keep the GPUs busy since they can be processed in parallel. The chunks are processed completely independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ff952ea-37ea-4901-8877-0c22e128badc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T23:11:28.881490Z",
     "iopub.status.busy": "2025-02-11T23:11:28.880829Z",
     "iopub.status.idle": "2025-02-11T23:11:28.897707Z",
     "shell.execute_reply": "2025-02-11T23:11:28.897018Z",
     "shell.execute_reply.started": "2025-02-11T23:11:28.881447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "\n",
      "yb:\n",
      " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # won't match video's since I'm using a different dataset\n",
    "block_size = 8   # the maximum context length for predictions\n",
    "batch_size = 4   # how many independent sequences will we process in parallel\n",
    "\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y;\n",
    "    # this will happen on any forward or backward pass through the model\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # get random off sets\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])        # then stack them all up together\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])    # needed to generate the loss function\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print('\\ntargets:')\n",
    "print(yb.shape)\n",
    "# the number of rows is the number of batches\n",
    "# the number of columns is the block_size\n",
    "print('\\nyb:\\n', yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "155905df-c7e6-473e-86da-7c3c796097dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T23:11:29.356915Z",
     "iopub.status.busy": "2025-02-11T23:11:29.355911Z",
     "iopub.status.idle": "2025-02-11T23:11:29.366768Z",
     "shell.execute_reply": "2025-02-11T23:11:29.365941Z",
     "shell.execute_reply.started": "2025-02-11T23:11:29.356839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
       "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
       "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
       "        [25, 17, 27, 10,  0, 21,  1, 54]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b58d04bb-5da3-4141-9c77-cd10f2351411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T23:11:41.728358Z",
     "iopub.status.busy": "2025-02-11T23:11:41.727659Z",
     "iopub.status.idle": "2025-02-11T23:11:41.737265Z",
     "shell.execute_reply": "2025-02-11T23:11:41.736360Z",
     "shell.execute_reply.started": "2025-02-11T23:11:41.728314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"et's hea\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again, each is a random chunk of data\n",
    "# represented by integer values. you can verify this here\n",
    "# this is just the first batch of `yb`\n",
    "decode(np.array(yb)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "961c7822-4740-4b8a-bfd8-c80153666c9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T23:11:35.491315Z",
     "iopub.status.busy": "2025-02-11T23:11:35.490313Z",
     "iopub.status.idle": "2025-02-11T23:11:35.503849Z",
     "shell.execute_reply": "2025-02-11T23:11:35.503170Z",
     "shell.execute_reply.started": "2025-02-11T23:11:35.491241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [24], the target is: 43\n",
      "when input is [24, 43], the target is: 58\n",
      "when input is [24, 43, 58], the target is: 5\n",
      "when input is [24, 43, 58, 5], the target is: 57\n",
      "when input is [24, 43, 58, 5, 57], the target is: 1\n",
      "when input is [24, 43, 58, 5, 57, 1], the target is: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46], the target is: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43], the target is: 39\n",
      "when input is [44], the target is: 53\n",
      "when input is [44, 53], the target is: 56\n",
      "when input is [44, 53, 56], the target is: 1\n",
      "when input is [44, 53, 56, 1], the target is: 58\n",
      "when input is [44, 53, 56, 1, 58], the target is: 46\n",
      "when input is [44, 53, 56, 1, 58, 46], the target is: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39], the target is: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58], the target is: 1\n",
      "when input is [52], the target is: 58\n",
      "when input is [52, 58], the target is: 1\n",
      "when input is [52, 58, 1], the target is: 58\n",
      "when input is [52, 58, 1, 58], the target is: 46\n",
      "when input is [52, 58, 1, 58, 46], the target is: 39\n",
      "when input is [52, 58, 1, 58, 46, 39], the target is: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58], the target is: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1], the target is: 46\n",
      "when input is [25], the target is: 17\n",
      "when input is [25, 17], the target is: 27\n",
      "when input is [25, 17, 27], the target is: 10\n",
      "when input is [25, 17, 27, 10], the target is: 0\n",
      "when input is [25, 17, 27, 10, 0], the target is: 21\n",
      "when input is [25, 17, 27, 10, 0, 21], the target is: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1], the target is: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54], the target is: 39\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):  # batch dimension\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "380dee74-3afa-42a0-aff3-d84921518399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[28, 33, 20, 21, 32,  1, 19, 30],\n",
      "        [24, 18,  1,  5, 21, 22, 27, 18],\n",
      "        [31,  1, 26, 18,  1, 28, 33, 32],\n",
      "        [30, 31,  1, 21, 18, 30,  1, 28]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eefab0b-9f30-4328-aca2-aba7b318f7f5",
   "metadata": {},
   "source": [
    "# Feed this into a simple language model\n",
    "\n",
    "Bigram language model is covered in his make more series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b484879-3d0d-4329-8d00-45a11d8b778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "# nn module is a basic blocking block for graphs \n",
    "# https://pytorch.org/docs/stable/nn.html\n",
    "class BigramLanguageModel(nn.Module):   # subclassing nn.Module\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        # for example token 30 (tensor above) will grab the 30th row of this\n",
    "        # embedding table, see output below\n",
    "        \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is x; inputs renamed to idx (not sure why it's renamed)\n",
    "        # targets is y;\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # logits are the predictions\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)  # what is channel? the character size\n",
    "\n",
    "        # use this conditional to evaluate the loss (quality of predictions)\n",
    "        # if it is possible\n",
    "        # likelihood maximization = minimization of neg log likelihood = minimization of the cross-entropy\n",
    "        # https://discuss.pytorch.org/t/difference-between-cross-entropy-loss-or-log-likelihood-loss/38816/2\n",
    "        if targets is None:  # this is needed for the `generate` function below where loss isn't used\n",
    "            loss = None\n",
    "        else:\n",
    "            # need to change order of dimensions so it can be read by `cross_entropy`\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # a 2D view\n",
    "            targets = targets.view(B*T)  # this will be 1D\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # generate text, by just adding one more for every batch dimension in the time dimension\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)  # loss is being ignored here\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the multinomial probability distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence... whatever is predicted is concatenated\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "80fb0edb-9359-464e-8f5e-9170141a4e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(37, 37)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the model; all we have is embedding\n",
    "# per the docs  https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding\n",
    "# A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "# it's a thin wrapper around a tensor of vocab_size x vocab_size\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "m.token_embedding_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d000b99a-0118-4379-9c64-aa898aeca328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 37])\n"
     ]
    }
   ],
   "source": [
    "# understanding this embedding table object\n",
    "# for example token 30 (tensor above) will grab the 30th row of this\n",
    "# embedding table\n",
    "# Single index lookup\n",
    "index = torch.tensor([30])\n",
    "embedding_vals_idx30 = m.token_embedding_table(index)\n",
    "print(embedding_vals_idx30.shape)  # Output: torch.Size([1, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ed25006b-77e2-44d1-b098-e24c5d9193b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9883,  0.5120,  0.8213, -0.4068,  1.1888, -1.4860,  0.8820,  0.1223,\n",
       "          0.4179, -0.6261, -0.7949,  0.8804, -0.6083,  0.4067,  0.1444,  0.7383,\n",
       "          0.2105,  0.7643,  0.6467,  0.6577,  0.3708, -0.4748,  0.5696,  0.8532,\n",
       "          2.7071,  0.7921, -0.3820, -0.5626,  1.5889,  0.7005, -0.6309, -0.5384,\n",
       "          2.2132, -0.6222, -0.6999, -0.2165,  2.0037]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vals_idx30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d7c469e-a368-4339-aa6f-ca1a1cb93e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = m(xb, yb)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0f65fff3-dbfe-4648-b508-c36aaee508fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 37])\n",
      "tensor(4.1957, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2226a53c-237e-4003-97c9-85759e640066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "u\n",
      "aYkgBnnA\n",
      "woMAlgevAWlHbyfvA aojhy\n",
      "CBaA mvdpSIClBsSts'HiWMTuBevBhncl'HiCyMAkbl'iWjMyWcoHpwBIdgkWMkcv\n"
     ]
    }
   ],
   "source": [
    "# test generation\n",
    "# idx is being set to a 1x1 tensor of 0\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77bd242-1734-42f6-a5ce-f601922f74c8",
   "metadata": {},
   "source": [
    "This is garbage because the model isn't trained.\n",
    "\n",
    "Also, the bigram model isn't using any history, other than the last character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1f17f-3345-42aa-9776-068810075d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419c456-eb3f-457b-89c5-83e4a092f656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
