{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df4c5739-4f87-4494-88a8-4e603ec1e31a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T14:35:57.594449Z",
     "iopub.status.busy": "2025-02-11T14:35:57.592425Z",
     "iopub.status.idle": "2025-02-11T14:37:27.888739Z",
     "shell.execute_reply": "2025-02-11T14:37:27.888166Z",
     "shell.execute_reply.started": "2025-02-11T14:35:57.594341Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57fba35e-d730-447d-a7fa-cc6e7522a693",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T14:37:27.890393Z",
     "iopub.status.busy": "2025-02-11T14:37:27.890120Z",
     "iopub.status.idle": "2025-02-11T14:37:27.893676Z",
     "shell.execute_reply": "2025-02-11T14:37:27.893192Z",
     "shell.execute_reply.started": "2025-02-11T14:37:27.890375Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30199d4a-6ee9-48a0-9819-25aed9079be0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T14:37:27.894885Z",
     "iopub.status.busy": "2025-02-11T14:37:27.894734Z",
     "iopub.status.idle": "2025-02-11T14:37:27.897459Z",
     "shell.execute_reply": "2025-02-11T14:37:27.897076Z",
     "shell.execute_reply.started": "2025-02-11T14:37:27.894868Z"
    }
   },
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f58f877-3c70-4d49-be9f-c3d9dd800b79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T14:37:27.899341Z",
     "iopub.status.busy": "2025-02-11T14:37:27.899164Z",
     "iopub.status.idle": "2025-02-11T14:37:28.753724Z",
     "shell.execute_reply": "2025-02-11T14:37:28.752898Z",
     "shell.execute_reply.started": "2025-02-11T14:37:27.899324Z"
    }
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# radiohead lyrics from Manik2000\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://raw.githubusercontent.com/Manik2000/radiohead-lyrics/main/data/lyrics.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFake Plastic Trees\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlyrics\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# cut out random notes at the start of the field and at the end\u001b[39;00m\n\u001b[1;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLyrics\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m88Embed\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/pandas/io/common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/pandas/io/common.py:384\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    383\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    385\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.12/site-packages/pandas/io/common.py:289\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.12/urllib/request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.12/urllib/request.py:521\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    520\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 521\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.12/urllib/request.py:630\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 630\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.12/urllib/request.py:559\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    558\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.12/urllib/request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch_env/lib/python3.12/urllib/request.py:639\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "# radiohead lyrics from Manik2000\n",
    "text = pd.read_csv(\"https://raw.githubusercontent.com/Manik2000/radiohead-lyrics/main/data/lyrics.csv\").query(\"title=='Fake Plastic Trees'\")['lyrics'].iloc[0]\n",
    "# cut out random notes at the start of the field and at the end\n",
    "text = text.split(\"Lyrics\\n\")[1].split(\"88Embed\")[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22ef1f30-dad2-43f7-8020-62fb689a40ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  805\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a83e81-4840-42d1-b13c-4df14617e52e",
   "metadata": {},
   "source": [
    "# Encoding of characters\n",
    "\n",
    "This is very simple. But there are other methods like `tiktoken`, `sentencepiece` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce77e60-5206-4bf5-ad03-04abaa0e5d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 37\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"vocab size: {vocab_size}\")\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }  # string to integer\n",
    "itos = { i:ch for i,ch in enumerate(chars) }  # integer to string\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "796d7f2c-2b8e-46c4-b6f6-57ec882381b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 14, 24, 18, 1, 29, 25, 14, 31, 32, 22, 16, 1, 32, 30, 18, 18, 31]\n",
      "fake plastic trees\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"fake plastic trees\"))\n",
    "print(decode(encode(\"fake plastic trees\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fa76945-bffb-4129-b10f-4bd15e7b522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " \"'\": 2,\n",
       " 'A': 3,\n",
       " 'B': 4,\n",
       " 'C': 5,\n",
       " 'F': 6,\n",
       " 'H': 7,\n",
       " 'I': 8,\n",
       " 'M': 9,\n",
       " 'S': 10,\n",
       " 'T': 11,\n",
       " 'W': 12,\n",
       " 'Y': 13,\n",
       " 'a': 14,\n",
       " 'b': 15,\n",
       " 'c': 16,\n",
       " 'd': 17,\n",
       " 'e': 18,\n",
       " 'f': 19,\n",
       " 'g': 20,\n",
       " 'h': 21,\n",
       " 'i': 22,\n",
       " 'j': 23,\n",
       " 'k': 24,\n",
       " 'l': 25,\n",
       " 'm': 26,\n",
       " 'n': 27,\n",
       " 'o': 28,\n",
       " 'p': 29,\n",
       " 'r': 30,\n",
       " 's': 31,\n",
       " 't': 32,\n",
       " 'u': 33,\n",
       " 'v': 34,\n",
       " 'w': 35,\n",
       " 'y': 36}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2af7482a-2f90-458a-ae91-665f094097cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and store it in a torch.tensor object\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # torch.long is desired memory format of Tensor\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b75e9125-7e19-42ae-9df0-fac6a1cc3dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A green plastic watering can\\nFor a fake Chinese rubber plant\\nIn a fake plastic earth\\nThat she bought from a rubber man\\nIn a town full of rubber plants\\nTo get rid of itself\\n\\nIt wears her out\\nIt wears her out\\nIt wears her out\\nIt wears her out\\n\\nShe lives with a broken man\\nA cracked polystyrene man\\nWho just crumbles and burns\\nHe used to do surgery\\nFor girls in the eighties\\nBut gravity always wins\\n\\nAnd it wears him out\\nIt wears him out\\nIt wears him out\\nIt wears\\nYou might also like\\nShe looks like the real thing\\nShe tastes like the real thing\\nMy fake plastic love\\nBut I can't help the feeling\\nI could blow through the ceiling\\nIf I just turn and run\\n\\nAnd it wears me out\\nIt wears me out\\nIt wears me out\\nIt wears me out\\n\\nAnd if I could be who you wanted\\nIf I could be who you wanted\\nAll the time\\nAll the time\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `data` is just numbers representing the lyrics\n",
    "decode(encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f57184-3ea6-4fe8-89d3-7a95d7ff375f",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "We don't feed the whole training data into the model. Too computationally expensive. Instead, we feed random chunks of data at a time. The size of these chunks is called  `block_size` in this example but others may call it `context_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "590fa106-8d6e-4975-9a27-bf38df414ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  1, 20, 30, 18, 18, 27,  1, 29])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c262bb88-892f-462d-88d6-9afbede2cb35",
   "metadata": {},
   "source": [
    "In this particular example, there's actually a lot of examples packed into it because of the masking.\n",
    "\n",
    "`3`\n",
    "<br>\n",
    "`3,  1`\n",
    "<br>\n",
    "`3,  1, 20`  # in the context of seeing `3, 1` then `20` comes next\n",
    "<br>\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2815a945-872c-4e97-bd43-2f5373deaf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is: tensor([3]) the target is: 1\n",
      "when input is: tensor([3, 1]) the target is: 20\n",
      "when input is: tensor([ 3,  1, 20]) the target is: 30\n",
      "when input is: tensor([ 3,  1, 20, 30]) the target is: 18\n",
      "when input is: tensor([ 3,  1, 20, 30, 18]) the target is: 18\n",
      "when input is: tensor([ 3,  1, 20, 30, 18, 18]) the target is: 27\n",
      "when input is: tensor([ 3,  1, 20, 30, 18, 18, 27]) the target is: 1\n",
      "when input is: tensor([ 3,  1, 20, 30, 18, 18, 27,  1]) the target is: 29\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is: {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890a5435-7b04-4924-9eee-2cd98dead641",
   "metadata": {},
   "source": [
    "This also helps the transformer get used to seeing contexts at these size ranges (from 1 to `block_size`). This is also helpful during inference so the model knows what sizes it can predict. Keep in mind that this also limits what the model can predict. After `block_size` the model has to truncate. The above is implicitly the time dimension, since a sequence where order matters is generated.\n",
    "\n",
    "Now consider the **batch** dimension. It's taking multiple random chunks of text and grouping them together to keep things efficient and keep the GPUs busy since they can be processed in parallel. The chunks are processed completely independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ff952ea-37ea-4901-8877-0c22e128badc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "\n",
      "yb:\n",
      " tensor([[33, 20, 21, 32,  1, 19, 30, 28],\n",
      "        [18,  1,  5, 21, 22, 27, 18, 31],\n",
      "        [ 1, 26, 18,  1, 28, 33, 32,  0],\n",
      "        [31,  1, 21, 18, 30,  1, 28, 33]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)  # won't match video's since I'm using a different dataset\n",
    "block_size = 8   # the maximum context length for predictions\n",
    "batch_size = 4   # how many independent sequences will we process in parallel\n",
    "\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y;\n",
    "    # this will happen on any forward or backward pass through the model\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # get random off sets\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])        # then stack them all up together\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])    # needed to generate the loss function\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print('\\ntargets:')\n",
    "print(yb.shape)\n",
    "# the number of rows is the number of batches\n",
    "# the number of columns is the block_size\n",
    "print('\\nyb:\\n', yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "155905df-c7e6-473e-86da-7c3c796097dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[28, 33, 20, 21, 32,  1, 19, 30],\n",
       "        [24, 18,  1,  5, 21, 22, 27, 18],\n",
       "        [31,  1, 26, 18,  1, 28, 33, 32],\n",
       "        [30, 31,  1, 21, 18, 30,  1, 28]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b58d04bb-5da3-4141-9c77-cd10f2351411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ught fro'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again, each is a random chunk of data or lyrics\n",
    "# represented by integer values. you can verify this here\n",
    "# this is just the first batch of `yb`\n",
    "decode(np.array(yb)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "961c7822-4740-4b8a-bfd8-c80153666c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [28], the target is: 33\n",
      "when input is [28, 33], the target is: 20\n",
      "when input is [28, 33, 20], the target is: 21\n",
      "when input is [28, 33, 20, 21], the target is: 32\n",
      "when input is [28, 33, 20, 21, 32], the target is: 1\n",
      "when input is [28, 33, 20, 21, 32, 1], the target is: 19\n",
      "when input is [28, 33, 20, 21, 32, 1, 19], the target is: 30\n",
      "when input is [28, 33, 20, 21, 32, 1, 19, 30], the target is: 28\n",
      "when input is [24], the target is: 18\n",
      "when input is [24, 18], the target is: 1\n",
      "when input is [24, 18, 1], the target is: 5\n",
      "when input is [24, 18, 1, 5], the target is: 21\n",
      "when input is [24, 18, 1, 5, 21], the target is: 22\n",
      "when input is [24, 18, 1, 5, 21, 22], the target is: 27\n",
      "when input is [24, 18, 1, 5, 21, 22, 27], the target is: 18\n",
      "when input is [24, 18, 1, 5, 21, 22, 27, 18], the target is: 31\n",
      "when input is [31], the target is: 1\n",
      "when input is [31, 1], the target is: 26\n",
      "when input is [31, 1, 26], the target is: 18\n",
      "when input is [31, 1, 26, 18], the target is: 1\n",
      "when input is [31, 1, 26, 18, 1], the target is: 28\n",
      "when input is [31, 1, 26, 18, 1, 28], the target is: 33\n",
      "when input is [31, 1, 26, 18, 1, 28, 33], the target is: 32\n",
      "when input is [31, 1, 26, 18, 1, 28, 33, 32], the target is: 0\n",
      "when input is [30], the target is: 31\n",
      "when input is [30, 31], the target is: 1\n",
      "when input is [30, 31, 1], the target is: 21\n",
      "when input is [30, 31, 1, 21], the target is: 18\n",
      "when input is [30, 31, 1, 21, 18], the target is: 30\n",
      "when input is [30, 31, 1, 21, 18, 30], the target is: 1\n",
      "when input is [30, 31, 1, 21, 18, 30, 1], the target is: 28\n",
      "when input is [30, 31, 1, 21, 18, 30, 1, 28], the target is: 33\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):  # batch dimension\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "380dee74-3afa-42a0-aff3-d84921518399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[28, 33, 20, 21, 32,  1, 19, 30],\n",
      "        [24, 18,  1,  5, 21, 22, 27, 18],\n",
      "        [31,  1, 26, 18,  1, 28, 33, 32],\n",
      "        [30, 31,  1, 21, 18, 30,  1, 28]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eefab0b-9f30-4328-aca2-aba7b318f7f5",
   "metadata": {},
   "source": [
    "# Feed this into a simple language model\n",
    "\n",
    "Bigram language model is covered in his make more series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b484879-3d0d-4329-8d00-45a11d8b778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "# nn module is a basic blocking block for graphs \n",
    "# https://pytorch.org/docs/stable/nn.html\n",
    "class BigramLanguageModel(nn.Module):   # subclassing nn.Module\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        # for example token 30 (tensor above) will grab the 30th row of this\n",
    "        # embedding table, see output below\n",
    "        \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is x; inputs renamed to idx (not sure why it's renamed)\n",
    "        # targets is y;\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        # logits are the predictions\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)  # what is channel? the character size\n",
    "\n",
    "        # use this conditional to evaluate the loss (quality of predictions)\n",
    "        # if it is possible\n",
    "        # likelihood maximization = minimization of neg log likelihood = minimization of the cross-entropy\n",
    "        # https://discuss.pytorch.org/t/difference-between-cross-entropy-loss-or-log-likelihood-loss/38816/2\n",
    "        if targets is None:  # this is needed for the `generate` function below where loss isn't used\n",
    "            loss = None\n",
    "        else:\n",
    "            # need to change order of dimensions so it can be read by `cross_entropy`\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # a 2D view\n",
    "            targets = targets.view(B*T)  # this will be 1D\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # generate text, by just adding one more for every batch dimension in the time dimension\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)  # loss is being ignored here\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the multinomial probability distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence... whatever is predicted is concatenated\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "80fb0edb-9359-464e-8f5e-9170141a4e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(37, 37)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the model; all we have is embedding\n",
    "# per the docs  https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding\n",
    "# A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "# it's a thin wrapper around a tensor of vocab_size x vocab_size\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "m.token_embedding_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d000b99a-0118-4379-9c64-aa898aeca328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 37])\n"
     ]
    }
   ],
   "source": [
    "# understanding this embedding table object\n",
    "# for example token 30 (tensor above) will grab the 30th row of this\n",
    "# embedding table\n",
    "# Single index lookup\n",
    "index = torch.tensor([30])\n",
    "embedding_vals_idx30 = m.token_embedding_table(index)\n",
    "print(embedding_vals_idx30.shape)  # Output: torch.Size([1, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ed25006b-77e2-44d1-b098-e24c5d9193b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9883,  0.5120,  0.8213, -0.4068,  1.1888, -1.4860,  0.8820,  0.1223,\n",
       "          0.4179, -0.6261, -0.7949,  0.8804, -0.6083,  0.4067,  0.1444,  0.7383,\n",
       "          0.2105,  0.7643,  0.6467,  0.6577,  0.3708, -0.4748,  0.5696,  0.8532,\n",
       "          2.7071,  0.7921, -0.3820, -0.5626,  1.5889,  0.7005, -0.6309, -0.5384,\n",
       "          2.2132, -0.6222, -0.6999, -0.2165,  2.0037]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vals_idx30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d7c469e-a368-4339-aa6f-ca1a1cb93e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = m(xb, yb)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0f65fff3-dbfe-4648-b508-c36aaee508fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 37])\n",
      "tensor(4.1957, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2226a53c-237e-4003-97c9-85759e640066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "u\n",
      "aYkgBnnA\n",
      "woMAlgevAWlHbyfvA aojhy\n",
      "CBaA mvdpSIClBsSts'HiWMTuBevBhncl'HiCyMAkbl'iWjMyWcoHpwBIdgkWMkcv\n"
     ]
    }
   ],
   "source": [
    "# test generation\n",
    "# idx is being set to a 1x1 tensor of 0\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77bd242-1734-42f6-a5ce-f601922f74c8",
   "metadata": {},
   "source": [
    "This is garbage because the model isn't trained.\n",
    "\n",
    "Also, the bigram model isn't using any history, other than the last character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1f17f-3345-42aa-9776-068810075d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419c456-eb3f-457b-89c5-83e4a092f656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
